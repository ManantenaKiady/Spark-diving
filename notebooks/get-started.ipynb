{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get started with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Spark and is it still worth learning in 2023 ?\n",
    "\n",
    "Spark is defined as unified analytics. \n",
    "\n",
    "- For data engineers with distributed data processing capabilities, SQL supported\n",
    "- For data analysts: with SparkSQL, Dataframe\n",
    "- For data scientist: SparkML, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/manantena/.virtualenvs/datascience/lib/python3.6/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/home/manantena/.virtualenvs/datascience/lib/python3.6/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: pyspark in /home/manantena/.virtualenvs/datascience/lib/python3.6/site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /home/manantena/.virtualenvs/datascience/lib/python3.6/site-packages (from pyspark) (0.10.9.3)\n"
     ]
    }
   ],
   "source": [
    "# For Python we can install spark or pyspark more precisely with pip\n",
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or download and build spark from source\n",
    "\n",
    "On ubuntu, we first need to update packages \n",
    "\n",
    "`sudo apt update && sudo apt -y full-upgrade`\n",
    "\n",
    "Then install Java Runtime and default-jdk\n",
    "\n",
    "`sudo apt install curl mlocate default-jdk -y`\n",
    "\n",
    "Next, download Apache Spark from the downloads page\n",
    "https://spark.apache.org/downloads.html\n",
    "\n",
    "Or\n",
    "\n",
    "`wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz`\n",
    "\n",
    "Extract and move spark to /opt/spark\n",
    "\n",
    "`tar xvf spark-3.2.1-bin-hadoop3.2.tgz`\n",
    "\n",
    "`sudo mv spark-3.2.1-bin-hadoop3.2/ /opt/spark `\n",
    "\n",
    "\n",
    "Then configure like so: Open the `~/.bachrc` file and add the following variables.\n",
    "\n",
    "\n",
    "`export SPARK_HOME=/opt/spark`\n",
    "`export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting spark\n",
    "\n",
    "To start a standalone master server run\n",
    "\n",
    "`start-master.sh`\n",
    "\n",
    "Chech the WebUI on <url> http://localhost:8080 </url>\n",
    "\n",
    "Note the spark URL is like: spark://ASUS-X442UA-KD:7077\n",
    "\n",
    "Then start a worker with \n",
    "\n",
    "`start-worker.sh spark://ASUS-X442UA-KD:7077`\n",
    "\n",
    "By default, spark will allocate all CPU when running a worker, to make a worker uses one CPu and 128 Mb of RAM, for exemple, we will do like so\n",
    "\n",
    "`start-worker.sh -c 1 -m 128M spark://ASUS-X442UA-KD:7077`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running spark-shell or pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/07 18:14:09 WARN Utils: Your hostname, ASUS-X442UA-KD resolves to a loopback address: 127.0.1.1; using 192.168.8.101 instead (on interface enx0c5b8f279a64)\n",
      "23/01/07 18:14:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/07 18:14:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Spark context Web UI available at http://192.168.8.101:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1673104459473).\n",
      "Spark session available as 'spark'.\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n",
      "      /_/\n",
      "         \n",
      "Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 11.0.9)\n",
      "Type in expressions to have them evaluated.\n",
      "Type :help for more information.\n",
      "\u001b[35m\n",
      "scala> \u001b[0m\n",
      "\u001b[35m\n",
      "scala> \u001b[0m"
     ]
    }
   ],
   "source": [
    "!spark-shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.6 (default, Sep 12 2018, 18:26:19) \n",
      "[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      "23/01/07 18:15:17 WARN Utils: Your hostname, ASUS-X442UA-KD resolves to a loopback address: 127.0.1.1; using 192.168.8.101 instead (on interface enx0c5b8f279a64)\n",
      "23/01/07 18:15:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/07 18:15:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/opt/spark/python/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.\n",
      "  FutureWarning\n",
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.2.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.6 (default, Sep 12 2018 18:26:19)\n",
      "Spark context Web UI available at http://192.168.8.101:4040\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1673104521947).\n",
      "SparkSession available as 'spark'.\n",
      ">>> \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/opt/spark/python/pyspark/context.py\", line 293, in signal_handler\n",
      "    raise KeyboardInterrupt()\n",
      "KeyboardInterrupt\n",
      ">>> "
     ]
    }
   ],
   "source": [
    "# Or pyspark\n",
    "!pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark cluster\n",
    "\n",
    "`stop-woker.sh`\n",
    "\n",
    "`stop-master.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: stop-worker.sh: not found\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec8fa012cdcbdcf2b41971c013367ececacaf9037a1913fab48a98c654dbbbc5"
  },
  "kernelspec": {
   "display_name": "Python 3.6.6 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
